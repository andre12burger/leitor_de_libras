---

```markdown
# Leitor de Libras com Reconhecimento de Gestos em Tempo Real

Este projeto tem como objetivo detectar e traduzir gestos da L√≠ngua Brasileira de Sinais (LIBRAS) para texto, reconhecendo letras isoladas em tempo real a partir de capturas de v√≠deo via webcam. O sistema utiliza landmarks das m√£os para identificar sinais e exibir a letra correspondente diretamente na tela.

## √çndice
- [Descri√ß√£o Geral](#descri√ß√£o-geral)
- [Tecnologias Utilizadas](#tecnologias-utilizadas)
- [Como Funciona](#como-funciona)
- [Execu√ß√£o do Projeto](#execu√ß√£o-do-projeto)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Exemplo Visual](#exemplo-visual)
- [Depend√™ncias e Instala√ß√£o](#depend√™ncias-e-instala√ß√£o)

---

## Descri√ß√£o Geral

Este sistema de leitura de LIBRAS funciona em tempo real utilizando a webcam. Um modelo de rede neural densa (MLP) foi treinado do zero para classificar gestos de letras do alfabeto com base nas posi√ß√µes das m√£os, capturadas por meio da biblioteca **MediaPipe**. O modelo classifica os gestos e retorna uma letra correspondente.

---

## Tecnologias Utilizadas

- [MediaPipe](https://developers.google.com/mediapipe): Para rastreamento das m√£os e extra√ß√£o de landmarks.
- [OpenCV](https://opencv.org/): Para captura e exibi√ß√£o das imagens da webcam.
- [NumPy](https://numpy.org/): Manipula√ß√£o num√©rica dos dados.
- [Pickle](https://docs.python.org/3/library/pickle.html): Salvamento e carregamento do modelo treinado.
- [tqdm](https://tqdm.github.io/): Barra de progresso usada na cria√ß√£o do dataset.

---

## Como Funciona

1. A webcam captura o v√≠deo em tempo real.
2. O MediaPipe extrai os pontos de refer√™ncia (landmarks) da m√£o.
3. Esses pontos s√£o usados como entrada para o modelo MLP treinado.
4. O modelo prediz uma letra do alfabeto correspondente ao gesto.
5. A letra √© exibida diretamente na tela, sobre o v√≠deo.

---

## Execu√ß√£o do Projeto

### Pr√©-requisitos

Crie um ambiente virtual (opcional) e instale as depend√™ncias com o comando:

```bash
pip install -r requirements.txt
```

### Rodando o projeto

No terminal, execute o script principal:

```bash
python main.py
```

Ao executar, a webcam ser√° ativada e as letras reconhecidas ser√£o exibidas na tela.

---


## Exemplo Visual

Abaixo, um exemplo do sistema em funcionamento:

![exemplo](images/exemplo.png)

> O gesto da m√£o foi reconhecido como a letra **A**.

---

## Depend√™ncias e Instala√ß√£o

As principais bibliotecas utilizadas est√£o listadas no arquivo `requirements.txt`. Para instalar todas elas:

```bash
pip install -r requirements.txt
```

Em caso de problemas com ambientes virtuais, considere usar o [Conda](https://docs.conda.io/en/latest/) ou recriar o ambiente com o Python compat√≠vel.

---

**‚ö†Ô∏è Observa√ß√£o:**  
O projeto n√£o depende das imagens originais capturadas, apenas dos arquivos `.model` e `.pkl` com os dados e o modelo j√° treinado. Por isso, imagens brutas n√£o est√£o inclu√≠das no reposit√≥rio.

---

‚ö†Ô∏è **Limita√ß√£o atual:**  
O modelo reconhece apenas letras que n√£o envolvem movimentos, ou seja, **A, B, C, D, E, F, G, I, L, M, N, O, P, Q, R, S, T, U, V, W, Y**.  
Letras que requerem **movimento din√¢mico** como **H, J, K, X, Z** ainda **n√£o est√£o implementadas**, pois exigem an√°lise temporal (sequ√™ncia de frames) e tratamento mais avan√ßado com s√©ries temporais ou modelos baseados em v√≠deo.

---

üìå *Este projeto √© uma demonstra√ß√£o de um sistema simples de reconhecimento de sinais em LIBRAS, √∫til como base para sistemas mais complexos de tradu√ß√£o de linguagem de sinais para texto ou fala.*

---